{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/monsoon-nlp/bert-base-thai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issueid</th>\n",
       "      <th>dekaid</th>\n",
       "      <th>year</th>\n",
       "      <th>category</th>\n",
       "      <th>issueno</th>\n",
       "      <th>lawids</th>\n",
       "      <th>fact</th>\n",
       "      <th>decision</th>\n",
       "      <th>isact</th>\n",
       "      <th>isexternalelements</th>\n",
       "      <th>isinternalelement</th>\n",
       "      <th>isintent</th>\n",
       "      <th>isneglect</th>\n",
       "      <th>iscause</th>\n",
       "      <th>isjustify</th>\n",
       "      <th>isexcuse</th>\n",
       "      <th>isguilty</th>\n",
       "      <th>isattempt</th>\n",
       "      <th>isattemptimpossible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1478/2528</td>\n",
       "      <td>2528</td>\n",
       "      <td>LB</td>\n",
       "      <td>1</td>\n",
       "      <td>CC-288-00,CC-083-00,CC-063-00</td>\n",
       "      <td>จำเลยกับพวกร่วมกันใช้อาวุธปืนยิงผู้ตายถูกที่ด้...</td>\n",
       "      <td>จำเลยจึงมีความผิดฐานฆ่าผู้ตายโดยเจตนา</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1548/2531</td>\n",
       "      <td>2531</td>\n",
       "      <td>LB</td>\n",
       "      <td>1</td>\n",
       "      <td>CC-288-00</td>\n",
       "      <td>จำเลยที่ 1 ซึ่งเคยมีเรื่องทะเลาะกับผู้ตายมาก่อ...</td>\n",
       "      <td>จำเลยที่ 1 จึงมีความผิดฐานฆ่าผู้ตายโดยเจตนา</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1548/2531</td>\n",
       "      <td>2531</td>\n",
       "      <td>LB</td>\n",
       "      <td>2</td>\n",
       "      <td>CC-290-00,CC-083-00</td>\n",
       "      <td>ส่วนจำเลยที่ 2 ที่ 3 และที่ 4 นั้น ได้ความว่าก...</td>\n",
       "      <td>การที่จำเลยที่ 1 ใช้เหล็กแหลมแทงผู้ตายโดยเจตนา...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1548/2531</td>\n",
       "      <td>2531</td>\n",
       "      <td>LB</td>\n",
       "      <td>3</td>\n",
       "      <td>CC-288-00,CC-083-00</td>\n",
       "      <td>ส่วนจำเลยที่ 2 ที่ 3 และที่ 4 นั้น ได้ความว่าก...</td>\n",
       "      <td>การที่จำเลยที่ 1 ใช้เหล็กแหลมแทงผู้ตายโดยเจตนา...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1697/2522</td>\n",
       "      <td>2522</td>\n",
       "      <td>LB</td>\n",
       "      <td>1</td>\n",
       "      <td>CC-288-00,CC-083-00</td>\n",
       "      <td>โจทก์บรรยายฟ้องว่า จำเลยกับพวกที่ยังไม่ได้ตัวม...</td>\n",
       "      <td>จึงเป็นการกระทำโดยมีเจตนาฆ่าผู้ตาย แม้ข้อเท็จจ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   issueid     dekaid  year category  issueno                         lawids  \\\n",
       "0        1  1478/2528  2528       LB        1  CC-288-00,CC-083-00,CC-063-00   \n",
       "1        2  1548/2531  2531       LB        1                      CC-288-00   \n",
       "2        3  1548/2531  2531       LB        2            CC-290-00,CC-083-00   \n",
       "3        4  1548/2531  2531       LB        3            CC-288-00,CC-083-00   \n",
       "4        5  1697/2522  2522       LB        1            CC-288-00,CC-083-00   \n",
       "\n",
       "                                                fact  \\\n",
       "0  จำเลยกับพวกร่วมกันใช้อาวุธปืนยิงผู้ตายถูกที่ด้...   \n",
       "1  จำเลยที่ 1 ซึ่งเคยมีเรื่องทะเลาะกับผู้ตายมาก่อ...   \n",
       "2  ส่วนจำเลยที่ 2 ที่ 3 และที่ 4 นั้น ได้ความว่าก...   \n",
       "3  ส่วนจำเลยที่ 2 ที่ 3 และที่ 4 นั้น ได้ความว่าก...   \n",
       "4  โจทก์บรรยายฟ้องว่า จำเลยกับพวกที่ยังไม่ได้ตัวม...   \n",
       "\n",
       "                                            decision  isact  \\\n",
       "0              จำเลยจึงมีความผิดฐานฆ่าผู้ตายโดยเจตนา      1   \n",
       "1        จำเลยที่ 1 จึงมีความผิดฐานฆ่าผู้ตายโดยเจตนา      1   \n",
       "2  การที่จำเลยที่ 1 ใช้เหล็กแหลมแทงผู้ตายโดยเจตนา...      1   \n",
       "3  การที่จำเลยที่ 1 ใช้เหล็กแหลมแทงผู้ตายโดยเจตนา...      0   \n",
       "4  จึงเป็นการกระทำโดยมีเจตนาฆ่าผู้ตาย แม้ข้อเท็จจ...      1   \n",
       "\n",
       "   isexternalelements  isinternalelement  isintent  isneglect  iscause  \\\n",
       "0                   1                  1         1         -1        1   \n",
       "1                   1                  1         1         -1        1   \n",
       "2                   1                  1         1         -1        1   \n",
       "3                  -1                 -1        -1         -1       -1   \n",
       "4                   1                  1         1         -1        1   \n",
       "\n",
       "   isjustify  isexcuse  isguilty  isattempt  isattemptimpossible  \n",
       "0          0         0         1          0                   -1  \n",
       "1          0         0         1          0                   -1  \n",
       "2          0         0         1          0                   -1  \n",
       "3         -1        -1         0         -1                   -1  \n",
       "4          0         0         1          0                   -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../datasets/tscc_v0.1-judgement.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694 267 246\n"
     ]
    }
   ],
   "source": [
    "''' Original Split '''\n",
    "year1, year2 = 2539, 2553\n",
    "\n",
    "''' My Split '''\n",
    "# year1, year2 = 2553, 2555\n",
    "\n",
    "train_df = df[df['year'] < year1]\n",
    "valid_df = df[df['year'].between(year1, year2)]\n",
    "test_df = df[df['year'] > year2]\n",
    "assert len(df) == len(train_df) + len(valid_df) + len(test_df)\n",
    "print(len(train_df), len(valid_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df['fact'].tolist()\n",
    "train_y = train_df['isguilty'].tolist()\n",
    "valid_x = valid_df['fact'].tolist()\n",
    "valid_y = valid_df['isguilty'].tolist()\n",
    "test_x = test_df['fact'].tolist()\n",
    "test_y = test_df['isguilty'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"airesearch/wangchanberta-base-att-spm-uncased\"\n",
    "#revision = \"finetuned@wisesight_sentiment\"\n",
    "revision = None\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, revision=revision\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                revision='main',\n",
    "                model_max_length=416,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSCCDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_x, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_x, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_x, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = TSCCDataset(train_encodings, train_y)\n",
    "valid_dataset = TSCCDataset(valid_encodings, valid_y)\n",
    "test_dataset = TSCCDataset(test_encodings, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./orig_task',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./orig_task_logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    #evaluation_strategy='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,        # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcfd1ce3b1f495b84bd692d92c5ceaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/870 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6684, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.11}\n",
      "{'loss': 0.6521, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.23}\n",
      "{'loss': 0.6766, 'learning_rate': 3e-06, 'epoch': 0.34}\n",
      "{'loss': 0.6774, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.46}\n",
      "{'loss': 0.6977, 'learning_rate': 5e-06, 'epoch': 0.57}\n",
      "{'loss': 0.6511, 'learning_rate': 6e-06, 'epoch': 0.69}\n",
      "{'loss': 0.6628, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6453, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.92}\n",
      "{'loss': 0.6745, 'learning_rate': 9e-06, 'epoch': 1.03}\n",
      "{'loss': 0.5948, 'learning_rate': 1e-05, 'epoch': 1.15}\n",
      "{'loss': 0.6936, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.26}\n",
      "{'loss': 0.7199, 'learning_rate': 1.2e-05, 'epoch': 1.38}\n",
      "{'loss': 0.5818, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.49}\n",
      "{'loss': 0.6167, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.61}\n",
      "{'loss': 0.6545, 'learning_rate': 1.5e-05, 'epoch': 1.72}\n",
      "{'loss': 0.6181, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.84}\n",
      "{'loss': 0.5876, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.95}\n",
      "{'loss': 0.5908, 'learning_rate': 1.8e-05, 'epoch': 2.07}\n",
      "{'loss': 0.6377, 'learning_rate': 1.9e-05, 'epoch': 2.18}\n",
      "{'loss': 0.5335, 'learning_rate': 2e-05, 'epoch': 2.3}\n",
      "{'loss': 0.6493, 'learning_rate': 2.1e-05, 'epoch': 2.41}\n",
      "{'loss': 0.6047, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.53}\n",
      "{'loss': 0.6392, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.64}\n",
      "{'loss': 0.6551, 'learning_rate': 2.4e-05, 'epoch': 2.76}\n",
      "{'loss': 0.4976, 'learning_rate': 2.5e-05, 'epoch': 2.87}\n",
      "{'loss': 0.5442, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.99}\n",
      "{'loss': 0.6638, 'learning_rate': 2.7000000000000002e-05, 'epoch': 3.1}\n",
      "{'loss': 0.5983, 'learning_rate': 2.8000000000000003e-05, 'epoch': 3.22}\n",
      "{'loss': 0.5093, 'learning_rate': 2.9e-05, 'epoch': 3.33}\n",
      "{'loss': 0.5627, 'learning_rate': 3e-05, 'epoch': 3.45}\n",
      "{'loss': 0.741, 'learning_rate': 3.1e-05, 'epoch': 3.56}\n",
      "{'loss': 0.5747, 'learning_rate': 3.2000000000000005e-05, 'epoch': 3.68}\n",
      "{'loss': 0.5339, 'learning_rate': 3.3e-05, 'epoch': 3.79}\n",
      "{'loss': 0.5362, 'learning_rate': 3.4000000000000007e-05, 'epoch': 3.91}\n",
      "{'loss': 0.5415, 'learning_rate': 3.5e-05, 'epoch': 4.02}\n",
      "{'loss': 0.3864, 'learning_rate': 3.6e-05, 'epoch': 4.14}\n",
      "{'loss': 0.4218, 'learning_rate': 3.7e-05, 'epoch': 4.25}\n",
      "{'loss': 0.4958, 'learning_rate': 3.8e-05, 'epoch': 4.37}\n",
      "{'loss': 0.388, 'learning_rate': 3.9000000000000006e-05, 'epoch': 4.48}\n",
      "{'loss': 0.3913, 'learning_rate': 4e-05, 'epoch': 4.6}\n",
      "{'loss': 0.4566, 'learning_rate': 4.1e-05, 'epoch': 4.71}\n",
      "{'loss': 0.5685, 'learning_rate': 4.2e-05, 'epoch': 4.83}\n",
      "{'loss': 0.3288, 'learning_rate': 4.3e-05, 'epoch': 4.94}\n",
      "{'loss': 0.3905, 'learning_rate': 4.4000000000000006e-05, 'epoch': 5.06}\n",
      "{'loss': 0.3156, 'learning_rate': 4.5e-05, 'epoch': 5.17}\n",
      "{'loss': 0.3956, 'learning_rate': 4.600000000000001e-05, 'epoch': 5.29}\n",
      "{'loss': 0.552, 'learning_rate': 4.7e-05, 'epoch': 5.4}\n",
      "{'loss': 0.3871, 'learning_rate': 4.8e-05, 'epoch': 5.52}\n",
      "{'loss': 0.4288, 'learning_rate': 4.9e-05, 'epoch': 5.63}\n",
      "{'loss': 0.2912, 'learning_rate': 5e-05, 'epoch': 5.75}\n",
      "{'loss': 0.499, 'learning_rate': 4.8648648648648654e-05, 'epoch': 5.86}\n",
      "{'loss': 0.4176, 'learning_rate': 4.72972972972973e-05, 'epoch': 5.98}\n",
      "{'loss': 0.3474, 'learning_rate': 4.594594594594595e-05, 'epoch': 6.09}\n",
      "{'loss': 0.2394, 'learning_rate': 4.4594594594594596e-05, 'epoch': 6.21}\n",
      "{'loss': 0.3501, 'learning_rate': 4.324324324324325e-05, 'epoch': 6.32}\n",
      "{'loss': 0.2707, 'learning_rate': 4.189189189189189e-05, 'epoch': 6.44}\n",
      "{'loss': 0.3121, 'learning_rate': 4.0540540540540545e-05, 'epoch': 6.55}\n",
      "{'loss': 0.3534, 'learning_rate': 3.918918918918919e-05, 'epoch': 6.67}\n",
      "{'loss': 0.236, 'learning_rate': 3.783783783783784e-05, 'epoch': 6.78}\n",
      "{'loss': 0.4814, 'learning_rate': 3.648648648648649e-05, 'epoch': 6.9}\n",
      "{'loss': 0.2651, 'learning_rate': 3.513513513513514e-05, 'epoch': 7.01}\n",
      "{'loss': 0.3558, 'learning_rate': 3.3783783783783784e-05, 'epoch': 7.13}\n",
      "{'loss': 0.2251, 'learning_rate': 3.2432432432432436e-05, 'epoch': 7.24}\n",
      "{'loss': 0.2693, 'learning_rate': 3.108108108108108e-05, 'epoch': 7.36}\n",
      "{'loss': 0.2233, 'learning_rate': 2.9729729729729733e-05, 'epoch': 7.47}\n",
      "{'loss': 0.3096, 'learning_rate': 2.8378378378378378e-05, 'epoch': 7.59}\n",
      "{'loss': 0.3305, 'learning_rate': 2.702702702702703e-05, 'epoch': 7.7}\n",
      "{'loss': 0.2699, 'learning_rate': 2.5675675675675675e-05, 'epoch': 7.82}\n",
      "{'loss': 0.2107, 'learning_rate': 2.4324324324324327e-05, 'epoch': 7.93}\n",
      "{'loss': 0.2791, 'learning_rate': 2.2972972972972976e-05, 'epoch': 8.05}\n",
      "{'loss': 0.1897, 'learning_rate': 2.1621621621621624e-05, 'epoch': 8.16}\n",
      "{'loss': 0.3122, 'learning_rate': 2.0270270270270273e-05, 'epoch': 8.28}\n",
      "{'loss': 0.2268, 'learning_rate': 1.891891891891892e-05, 'epoch': 8.39}\n",
      "{'loss': 0.2023, 'learning_rate': 1.756756756756757e-05, 'epoch': 8.51}\n",
      "{'loss': 0.2026, 'learning_rate': 1.6216216216216218e-05, 'epoch': 8.62}\n",
      "{'loss': 0.3256, 'learning_rate': 1.4864864864864867e-05, 'epoch': 8.74}\n",
      "{'loss': 0.1911, 'learning_rate': 1.3513513513513515e-05, 'epoch': 8.85}\n",
      "{'loss': 0.2454, 'learning_rate': 1.2162162162162164e-05, 'epoch': 8.97}\n",
      "{'loss': 0.2229, 'learning_rate': 1.0810810810810812e-05, 'epoch': 9.08}\n",
      "{'loss': 0.1205, 'learning_rate': 9.45945945945946e-06, 'epoch': 9.2}\n",
      "{'loss': 0.2239, 'learning_rate': 8.108108108108109e-06, 'epoch': 9.31}\n",
      "{'loss': 0.1511, 'learning_rate': 6.7567567567567575e-06, 'epoch': 9.43}\n",
      "{'loss': 0.2385, 'learning_rate': 5.405405405405406e-06, 'epoch': 9.54}\n",
      "{'loss': 0.1337, 'learning_rate': 4.0540540540540545e-06, 'epoch': 9.66}\n",
      "{'loss': 0.2273, 'learning_rate': 2.702702702702703e-06, 'epoch': 9.77}\n",
      "{'loss': 0.2411, 'learning_rate': 1.3513513513513515e-06, 'epoch': 9.89}\n",
      "{'loss': 0.1969, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 2429.6438, 'train_samples_per_second': 2.856, 'train_steps_per_second': 0.358, 'train_loss': 0.43427459988100775, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=870, training_loss=0.43427459988100775, metrics={'train_runtime': 2429.6438, 'train_samples_per_second': 2.856, 'train_steps_per_second': 0.358, 'train_loss': 0.43427459988100775, 'epoch': 10.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66afaac0c0f04665af8737c3cebc7815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7310730218887329,\n",
       " 'eval_accuracy': 0.7865168539325843,\n",
       " 'eval_f1': 0.7859282911116425,\n",
       " 'eval_precision': 0.7955623306233062,\n",
       " 'eval_recall': 0.8039716641505052,\n",
       " 'eval_runtime': 34.041,\n",
       " 'eval_samples_per_second': 7.843,\n",
       " 'eval_steps_per_second': 0.999,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_eval = trainer.evaluate()\n",
    "result_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/2/tokenizer_config.json',\n",
       " './models/2/special_tokens_map.json',\n",
       " './models/2/sentencepiece.bpe.model',\n",
       " './models/2/added_tokens.json',\n",
       " './models/2/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"./models/2/\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
